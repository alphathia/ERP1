## **1.1 Purpose and Use Statement**

This taxonomy is designed to support three interconnected decision-making contexts in the rapidly evolving field of agentic AI. First, it enables **researchers** to systematically classify and compare agentic AI systems, identify capability gaps, and develop theoretically grounded research agendas by providing a common vocabulary and conceptual framework that bridges technical capabilities with organizational outcomes. Second, the taxonomy supports **practitioners and architects** in making informed design and implementation decisions—from selecting appropriate architectural patterns to evaluating build-versus-buy trade-offs—by mapping capability requirements to maturity indicators and expected value outcomes. Third, it equips **product owners and enterprise leaders** with a structured approach for portfolio assessment, investment prioritization, and technology roadmapping by connecting technical capability dimensions to measurable organizational value categories.

The taxonomy distinguishes itself from existing frameworks in the corpus in several important ways. Unlike the Morris et al. (2024) AGI Levels framework, which focuses narrowly on performance and generality dimensions relative to human benchmarks, this CMV taxonomy explicitly incorporates organizational value realization and implementation maturity as core dimensions. Similarly, while the Mohammadi et al. (2025) evaluation taxonomy provides excellent coverage of *what* to evaluate (agent behavior, capabilities, reliability, safety) and *how* to evaluate it, our framework extends this by integrating *why* these capabilities matter from a value-creation perspective. The OpenAI Preparedness Framework addresses safety thresholds and safeguards but does not systematically connect these to broader capability dimensions or maturity stages. By synthesizing insights from agent design patterns (Liu et al., 2025), human-agent team dynamics (Iftikhar et al., 2024), enterprise deployment challenges (Mohammadi et al., 2025; McKinsey 2025), and the Agentic ROI perspective (Liu et al., 2025), this taxonomy offers a uniquely integrated lens for evaluating agentic AI systems across their full lifecycle.

The framework's practical utility lies in its capacity to answer questions such as: "What capabilities are required to achieve specific value outcomes?", "At what maturity level should we expect particular value realization?", and "What architectural patterns and organizational investments are needed to progress along the capability-maturity continuum?"

---

## **1.2 Meta-Characteristic**

> **"The capacity of an agentic AI system to generate measurable organizational value—through revenue enhancement, cost reduction, risk mitigation, productivity improvement, decision quality enhancement, safety assurance, or innovation enablement—as determined by its technical capabilities, implementation maturity, and effective integration with human-organizational systems."**

### Justification

This meta-characteristic was derived through systematic analysis of the project corpus and is justified on three grounds:

**1. Alignment with Project Scope (Agentic AI in Organizational/Enterprise Contexts):**
The corpus repeatedly emphasizes the enterprise deployment context. Liu et al. (2025) explicitly frame "Agentic ROI" as the central usability determinant, defining it as the information gain provided by an agent normalized by interaction costs and expenses. The McKinsey (2025) State of AI report documents that high-performing organizations achieve more than 5% EBIT attribution to AI use and are nearly three times as likely to fundamentally redesign workflows. Mohammadi et al. (2025) dedicate an entire section to enterprise-specific challenges including role-based access control, reliability guarantees, and compliance requirements. This value orientation ensures the taxonomy remains actionable for practitioners making real deployment decisions.

**2. Synthesis of Key Corpus Themes:**
The meta-characteristic integrates three major thematic clusters from the corpus:
- *Technical Capability Themes*: Reasoning and planning (Sun et al., 2025; Hatalis et al., 2023), tool use and memory (Mohammadi et al., 2025), multi-agent coordination (Iftikhar et al., 2024; Maldonado et al., 2024), and architectural patterns (Liu et al., 2025)
- *Maturity and Deployment Themes*: Performance levels from Emerging to Superhuman (Morris et al., 2024), deployment stages from experimental to scaled (McKinsey 2025), and capability thresholds requiring different safeguards (OpenAI Preparedness Framework)
- *Value Realization Themes*: Cost reduction and productivity gains (McKinsey 2025), innovation and workflow transformation (Sarferaz, 2025), risk mitigation and safety (OpenAI Preparedness Framework; Acharya et al., 2025)

**3. Constraining Effect on Dimension/Characteristic Selection:**
This meta-characteristic operates as a filter: dimensions and characteristics are *only* included if they demonstrably influence the system's capacity to generate organizational value. For example:
- A capability dimension like "reasoning depth" is included because it directly affects decision quality and task completion success rates
- A maturity dimension like "human-agent collaboration patterns" is included because it moderates value realization (Iftikhar et al., 2024)
- Purely technical attributes without demonstrated value linkage (e.g., parameter count per se) would be excluded

[ASSUMPTION]: The seven value categories (revenue enhancement, cost reduction, risk mitigation, productivity improvement, decision quality enhancement, safety assurance, innovation enablement) are treated as comprehensive. This assumption is based on synthesis of McKinsey (2025), Liu et al. (2025), and Sarferaz (2025), but may require user validation.

---

## **1.3 Objects of Classification**

### Object Definition

The **objects of classification** in this taxonomy are **agentic AI capability configurations**—that is, distinct combinations of technical capabilities, architectural patterns, and operational contexts that represent identifiable and distinguishable instances of agentic AI systems as described in the literature. These objects may manifest as:

1. **Specific agent systems/frameworks** described in the literature (e.g., AutoGPT, ReAct agents)
2. **Architectural patterns** that define how capabilities are configured (e.g., role-based cooperation, debate-based cooperation)
3. **Capability profiles** representing characteristic bundles of agent abilities (e.g., tool-augmented reasoning agents, multi-agent collaboration systems)
4. **Deployment scenarios** representing contextualized applications (e.g., IT operations agents, software engineering assistants)

This inclusive definition ensures the taxonomy can classify both abstract capability types and concrete instantiations, supporting both theoretical research and practical evaluation.

---

### Reference Objects Table

| ID | Object Name | Type | Source Document(s) | Brief Description | Relevance to CMV |
|----|-------------|------|-------------------|-------------------|------------------|
| O1 | ReAct Agent | Framework/Pattern | Sun et al. (2025); Mohammadi et al. (2025) | Agents that interleave reasoning and action steps, alternating between thought processes and tool use in iterative loops | Core reasoning-action integration pattern; high relevance to decision quality and task completion metrics |
| O2 | AutoGPT | System | Liu et al. (2025); Hatalis et al. (2023) | Autonomous agent that decomposes goals into subtasks, executes via terminal commands and APIs, with memory-based task tracking | Exemplar of goal-seeking autonomous agent; relevant to productivity and automation value |
| O3 | BabyAGI | System | Liu et al. (2025) | Task-driven autonomous agent that creates, prioritizes, and executes tasks to achieve objectives | Simple reference implementation for basic autonomous capabilities |
| O4 | Generative Agents (Stanford) | System | Hatalis et al. (2023) | Virtual characters combining LLMs with memory, planning, and reflection in sandbox environments | Benchmark for social simulation and emergent behavior capabilities |
| O5 | AgentBench Environments | Benchmark Suite | Liu et al. (2025) | Multi-dimensional evaluation across 8 environments: OS, Database, Knowledge Graph, Games, Web | Standard for evaluating agent capabilities across diverse task types |
| O6 | Role-based Cooperation Pattern | Architectural Pattern | Liu et al. (2025) | Hierarchical coordination where agents assume distinct roles (planner, assigner, worker, creator) | Relevant to multi-agent coordination maturity and division of labor value |
| O7 | Debate-based Cooperation Pattern | Architectural Pattern | Liu et al. (2025) | Multiple agents provide feedback and debate until consensus, refining responses iteratively | Addresses decision quality through diverse perspectives and verification |
| O8 | Passive Goal Creator | Pattern | Liu et al. (2025) | Agent waits for user to provide goals explicitly before acting | Lower autonomy baseline; relevant to understanding human-in-the-loop configurations |
| O9 | Proactive Goal Creator | Pattern | Liu et al. (2025) | Agent anticipates user needs and proposes goals without explicit instruction | Higher autonomy configuration; relevant to productivity and user experience value |
| O10 | AIOps LLM Agents | Application Domain | Zhang et al. (2026) | Agents for IT operations: failure detection, root cause analysis, auto-remediation in distributed systems | Direct cost reduction and availability value in enterprise IT |
| O11 | Software Engineering Agents | Application Domain | McKinsey (2025); Liu et al. (2025) | Agents for code generation, debugging, and development workflow assistance (e.g., GitHub Copilot, Cursor) | Highest current Agentic ROI domain per corpus; productivity value |
| O12 | ERP-Integrated Agents | Application Domain | Sarferaz (2025) | Agents embedded in enterprise resource planning systems for natural language interaction and content automation | Enterprise value through process automation and knowledge management |
| O13 | Human-Agent Teams (HATs) | Collaboration Model | Iftikhar et al. (2024); Mu et al. (2024) | Teams combining human members with intelligent agents, with various coordination mechanisms | Critical for understanding value mediation through human-AI collaboration |
| O14 | Deep Research Agents | System | Liu et al. (2025); McKinsey (2025) | Agents designed for extended research tasks with multiple search iterations and synthesis (e.g., OpenAI Deep Research) | High information quality, high agent time trade-off; research productivity value |
| O15 | Agent Evaluator Pattern | Pattern | Liu et al. (2025) | Dedicated evaluator component assessing agent performance for functional suitability and adaptability | Relevant to reliability maturity and continuous improvement value |
| O16 | MRKL Architecture | Framework | Hatalis et al. (2023) | Modular Reasoning, Knowledge, and Language system combining LLMs with expert modules (neural and symbolic) | Neuro-symbolic integration for enhanced reasoning capability |
| O17 | Long-Horizon Simulation Agents | Evaluation Context | Mohammadi et al. (2025) | Agents evaluated across extended multi-day interactions in simulated environments | Addresses memory maturity and consistency dimensions |
| O18 | Enterprise RBAC-Compliant Agents | Deployment Scenario | Mohammadi et al. (2025) | Agents operating within role-based access control constraints in enterprise environments | Security and compliance maturity; risk mitigation value |
| O19 | Safeguarded High-Capability Models | Safety Configuration | OpenAI Preparedness Framework (2025) | Models at "High" capability threshold with robustness, monitoring, and trust-based access safeguards | Safety assurance value; critical threshold for enterprise deployment |
| O20 | Agent Workflow Systems | Architecture Type | Liu et al. (2025) | Predefined paths coordinating LLMs and tools with deterministic planning and human-in-the-loop decisions | Controllability and predictability value; lower autonomy but higher reliability |
| O21 | End-to-End Agent Models | Architecture Type | Liu et al. (2025) | Agents trained via reinforcement learning to internalize planning and action-control without predefined workflows | Higher autonomy, higher capability ceiling; emerging maturity level |
| O22 | Cross-Reflection Pattern | Pattern | Liu et al. (2025) | Agent queries multiple other agents for feedback, integrating diverse perspectives | Decision quality enhancement through verification |
| O23 | Tool/Agent Registry Pattern | Pattern | Liu et al. (2025) | Registry enabling discovery and employment of agents/tools with different roles | Scalability and extensibility maturity indicator |
| O24 | Memory-Enhanced Agent | Capability Profile | Hatalis et al. (2023); Mohammadi et al. (2025) | Agents with explicit long-term memory systems (episodic, semantic, procedural) beyond context window | Relevant to consistency, context retention, and long-horizon task value |
| O25 | Industry 4.0 AgentAI | Application Domain | Piccialli et al. (2025) | Autonomous agents in manufacturing, energy, healthcare, networking for distributed AI operations | Cross-sector value realization in industrial contexts |

---

## **Corpus Coverage Notes**

### Documents Most Relevant to Capability Extraction:
- **Liu et al. (2025) "Agent design pattern catalogue"** — 18 architectural patterns with forces/trade-offs analysis
- **Mohammadi et al. (2025) "Evaluation and Benchmarking of LLM Agents"** — Comprehensive capability taxonomy (tool use, planning, reasoning, memory, multi-agent)
- **Sun et al. (2025) "A Survey of Reasoning with Foundation Models"** — Deep coverage of reasoning capabilities and agent reasoning paradigms
- **Hatalis et al. (2023) "Memory Matters"** — Detailed treatment of memory architectures and cognitive foundations
- **Acharya et al. (2025) "Agentic AI: A Comprehensive Survey"** — Capability foundations (autonomy, adaptability, goal-directedness)
- **Liu et al. (2025) "AgentBench"** — Multi-environment benchmark providing operational capability definitions

### Documents Most Relevant to Value/Maturity:
- **Liu et al. (2025) "The Real Barrier to LLM Agent Usability is Agentic ROI"** — Central value framework linking capability to practical usability
- **McKinsey (2025) "The State of AI in 2025"** — Enterprise adoption data, cost/revenue impacts, workflow transformation
- **Morris et al. (2024) "Levels of AGI"** — Performance/generality leveling system as maturity proxy
- **OpenAI Preparedness Framework (2025)** — Safety threshold framework with maturity implications
- **Sarferaz (2025) "Implementing Generative AI Into ERP Software"** — Enterprise value realization contexts
- **Iftikhar et al. (2024) "Human-Agent Team Dynamics"** — Value mediation through team composition and collaboration

### Potential Gaps in Corpus Coverage:
1. **Quantitative ROI data**: While Liu et al. (2025) proposes the Agentic ROI framework and McKinsey provides survey data, detailed case studies with specific financial outcomes are limited
2. **Longitudinal maturity progression**: Most sources provide snapshot assessments rather than documented progression paths through maturity levels
3. **Cross-industry value benchmarks**: Industrial applications are discussed (Piccialli et al., 2025) but standardized cross-industry value metrics are sparse
4. **Regulatory compliance maturity**: While mentioned (Mohammadi et al., 2025; Acharya et al., 2025), detailed compliance maturity models are underdeveloped in the corpus
5. **Small/medium enterprise contexts**: Corpus is weighted toward large enterprise and research contexts
